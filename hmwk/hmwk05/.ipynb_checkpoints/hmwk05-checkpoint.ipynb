{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Naive Bayes, Cross-Validation, and VC Dimension \n",
    "***\n",
    "\n",
    "**Name**: \n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Wednesday April 18th**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:42:01.964923Z",
     "start_time": "2018-04-03T14:42:01.249718Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 1 - Naive Bayes for Tennis Prediction \n",
    "***\n",
    "\n",
    "Suppose you are trying to learn a person's decision whether to play tennis or not on a given day using features corresponding to precipitation forecast, temperature, humidity, and wind. You're given the following training data: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "\\textbf{Forecast} & \\textbf{Temp} & \\textbf{Humidity} & \\textbf{Wind} & \\textbf{PlayTennis} \\\\\n",
    "\\hline \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Part A**: Estimate the priors $p(\\textrm{PlayTennis=Yes})$ and $p(\\textrm{PlayTennis=No})$ from the training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "Number of No = 5  \n",
    "Number of Yes = 9  \n",
    "Total = 14  \n",
    "P(Yes) = $\\frac{9}{14}$  \n",
    "P(No) = $\\frac{5}{14}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For each feature\n",
    "- state the vocabulary $V$ for the feature \n",
    "- estimate the class-conditional probabilities $p(\\textrm{feature value} \\mid \\textrm{PlayTennis})$ from the training data using Laplace add-one smoothing. Show your work. \n",
    "\n",
    "**Note**: There is no need to include any `UNK` features for this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Feature: Forecast  \n",
    "Vocabulary: Sunny, Overcast, Rainy  \n",
    "Number of Sunny and Yes = 2  \n",
    "Number of Overcast and Yes = 4  \n",
    "Number of Rainy and Yes = 3  \n",
    "Number of Sunny and No = 3  \n",
    "Number of Overcast and No = 0  \n",
    "Number of Rainy and No = 2  \n",
    "Apply add one smoothing and divide by P(Yes)  \n",
    "P(Sunny|Yes) = $\\frac{3}{12}$  \n",
    "P(Overcast|Yes) = $\\frac{5}{12}$  \n",
    "P(Sunny|Yes) = $\\frac{4}{12}$  \n",
    "  \n",
    "Apply add one smoothing and divide by P(No)  \n",
    "P(Sunny|No) = $\\frac{4}{8}$  \n",
    "P(Overcast|Yes) = $\\frac{1}{8}$  \n",
    "P(Sunny|Yes) = $\\frac{3}{8}$  \n",
    "  \n",
    "\n",
    "Feature: Temp  \n",
    "Vocabulary: Hot, mild, cool  \n",
    "Number of Hot and Yes = 2  \n",
    "Number of mild and Yes = 4  \n",
    "Number of cool and Yes = 3  \n",
    "Number of Hot and No = 2  \n",
    "Number of mild and No = 2  \n",
    "Number of cool and No = 1  \n",
    "Apply add one smoothing and divide by P(Yes)  \n",
    "P(Hot|Yes) = $\\frac{3}{12}$  \n",
    "P(mild|Yes) = $\\frac{5}{12}$  \n",
    "P(cool|Yes) = $\\frac{4}{12}$  \n",
    "  \n",
    "Apply add one smoothing and divide by P(No)  \n",
    "P(Hot|No) = $\\frac{3}{8}$  \n",
    "P(mild|No) = $\\frac{3}{8}$  \n",
    "P(cool|No) = $\\frac{2}{8}$  \n",
    "  \n",
    "  \n",
    "Feature: Humidity  \n",
    "Vocabulary: high, normal  \n",
    "Number of high and Yes = 3  \n",
    "Number of normal and Yes = 6  \n",
    "Number of high and No = 4  \n",
    "Number of normal and No = 1  \n",
    "Apply add one smoothing and divide by P(Yes)  \n",
    "P(high|Yes) = $\\frac{4}{11}$  \n",
    "P(normal|Yes) = $\\frac{7}{11}$  \n",
    "  \n",
    "Apply add one smoothing and divide by P(No)  \n",
    "P(high|No) = $\\frac{5}{7}$  \n",
    "P(normal|No) = $\\frac{2}{7}$  \n",
    "  \n",
    "\n",
    "Feature: Wind  \n",
    "Vocabulary: weak, strong    \n",
    "Number of weak and yes = 6  \n",
    "Number of strong and yes = 3  \n",
    "Number of weak and no = 2  \n",
    "Number of strong and no = 3  \n",
    "Apply add one smoothing and divide by P(Yes)  \n",
    "P(weak|Yes) = $\\frac{7}{11}$  \n",
    "P(strong|Yes) = $\\frac{4}{11}$  \n",
    "  \n",
    "Apply add one smoothing and divide by P(No)  \n",
    "P(weak|No) = $\\frac{3}{7}$  \n",
    "P(strong|No) = $\\frac{4}{7}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: What would your Naive Bayes model predict for the following two weather conditions. Show all work.  \n",
    "- **Forecast**=overcast, **Temp**=cool, **Humidity**=high, **Wind**=weak  \n",
    "- **Forecast**=sunny, **Temp**=hot, **Humidity**=normal, **Wind**=strong  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$P(Yes|overcast + cool + high + weak) \\approx P(overcast + cool + high + weak|Yes)P(Yes) = \\frac{5}{12}\\frac{4}{12}\\frac{4}{11}\\frac{7}{11}\\frac{9}{14} = 0.0206$  \n",
    "$P(No|overcast + cool + high + weak) \\approx P(overcast + cool + high + weak|No)P(No) = \\frac{1}{8}\\frac{2}{8}\\frac{5}{7}\\frac{3}{7}\\frac{5}{14} = 0.00341$  \n",
    "$0.0206 > 0.00341$ so classify as a day to play  \n",
    "$P(Yes|sunny + hot + normal + strong) \\approx P(sunny + hot + normal + strong|Yes)P(Yes) = \\frac{3}{12}\\frac{3}{12}\\frac{7}{11}\\frac{4}{11}\\frac{9}{14} = 0.00929$  \n",
    "$P(No|sunny + hot + normal + strong) \\approx P(sunny + hot + normal + strong|No)P(No) = \\frac{4}{8}\\frac{3}{8}\\frac{2}{7}\\frac{4}{7}\\frac{5}{14} = 0.0109$  \n",
    "$0.0109 > 0.00929$ so classify as a day to not play  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [35 points] Problem 2 - Implementing Discrete Naive Bayes for Text Classification \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general Discrete Naive Bayes class for text classification. Your tasks will be to implement `train`, `predict_log_score`, and `predict` routines to learn the Naive Bayes model parameters from a collection on text and make predictions on unseen validation data. \n",
    "\n",
    "The skeleton for the `TextNB` class is below. Note that this class is fairly similar to the one you worked with in the Hands-On Naive Bayes in-class notebook, so you should look there to remind yourself of the details. Scroll down to find more information about your tasks as well as unit tests.\n",
    "\n",
    "**Important Note**: In Problem 3 we'll be using the `TextNB` class to make predictions about Twitter data.  Since real-world text data typically has a large number of features, you'll want to make your implementation reasonably efficient so that your experiments in Problem 3 don't take forever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:42:09.159492Z",
     "start_time": "2018-04-03T14:42:09.076713Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextNB:\n",
    "    def __init__(self, text_train, y_train, alpha=1.0):\n",
    "        \"\"\"\n",
    "        :param text_train: a list or ndarray of text strings to use as training data \n",
    "        :param y_train: an ndarray of true labels associated with the text data \n",
    "        :param alpha: the Laplace smoothing parameter \n",
    "        \"\"\"\n",
    "        \n",
    "        # store training data \n",
    "        self.text_train = text_train \n",
    "        self.y_train = y_train \n",
    "        \n",
    "        # store smoothing parameter\n",
    "        self.alpha = alpha \n",
    "        \n",
    "        # get number of classes \n",
    "        self.num_classes = len(set(y_train))\n",
    "        \n",
    "        # initialize vocab to feature map \n",
    "        self.vocab = dict() \n",
    "        \n",
    "        # initialize class counts \n",
    "        self.class_counts = np.zeros(self.num_classes, dtype=int)\n",
    "        \n",
    "        # initialize feature counts (Note, will need to update this with the correct\n",
    "        # number of columns during the training process)\n",
    "        self.feature_counts = np.zeros((self.num_classes, 0), dtype=int)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn the vocabularly, class_counts, and feature counts from the training data \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "        numVocab = 0\n",
    "        for t in self.text_train:\n",
    "            w = t.split()\n",
    "            for w1 in w:\n",
    "                if(w1 not in self.vocab):\n",
    "                    self.vocab[w1] = numVocab\n",
    "                    numVocab += 1\n",
    "        \n",
    "        # initialize feature counts \n",
    "        classes = list(set(self.y_train))\n",
    "        for i in self.y_train:\n",
    "            self.class_counts[classes.index(i)] += 1\n",
    "            \n",
    "        self.feature_counts = np.zeros((self.num_classes, len(self.vocab)), dtype=int)\n",
    "        \n",
    "        for i,t in enumerate(self.text_train):\n",
    "            w = t.split()\n",
    "            for x in w:\n",
    "                self.feature_counts[classes.index(self.y_train[i])][self.vocab[x]] += 1\n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "    def predict_log_score(self, text_str):\n",
    "        \"\"\"\n",
    "        Get the log-probability score for each class\n",
    "        for a query string\n",
    "        \n",
    "        :param text_str: a single string of text to compute the log_score for \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "        \n",
    "        class_scores = np.zeros(self.num_classes) \n",
    "        words = text_str.split()\n",
    "        for i,c in enumerate(self.class_counts):\n",
    "            class_scores[i] *= (c+1)/(sum(self.class_counts)+self.num_classes)\n",
    "        for i in range(self.num_classes):\n",
    "            for w in words:\n",
    "                class_scores[i] *= (self.feature_counts[i][self.vocab[w]]+1)/(sum(self.feature_counts[i])+len(self.vocab))\n",
    "        return class_scores\n",
    "        \n",
    "    \n",
    "    def predict(self, text_list):\n",
    "        \"\"\"\n",
    "        Predict the class of each example in text_list  \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "    \n",
    "        yhat = np.zeros(len(text_list), dtype=int)\n",
    "        \n",
    "        for t in text_list:\n",
    "            yhat[x] = np.argmax(self.predict_log_score(t))\n",
    "        return yhat \n",
    "        \n",
    "        \n",
    "    def accuracy(self, text_list, y_true):\n",
    "        \"\"\"\n",
    "        Make predictions on texts in text_list and compute accuracy relative to \n",
    "        true labels in y_true \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        :param y_list: an ndarray of true labels associated with the text data \n",
    "        \"\"\"\n",
    "        yhat = self.predict(text_list)\n",
    "        return np.sum(yhat == y_true)/len(y_true)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `train` function in the `TextNB` class to prepare to make Naive Bayes predictions using Laplace smoothing.  In this routine you will need to populate the following data structures: \n",
    "\n",
    "`self.vocab`: A Python dictionary that maps distinct terms found in the training set to unique indices in $\\{0, 1, \\ldots, |V|-1\\}$.  This will allow us to quickly look up frequency counts for an encountered term in a Numpy array. Note that while the data is fairly clean (We've removed punctuation and made all characters lowercase) you should take care that you're not accidentally counting whitespace in the vocabulary. \n",
    "\n",
    "`self.class_counts`: A 1D Numpy array of length `self.num_classes` which counts the number of documents in the training set that belong to each class. \n",
    "\n",
    "`self.feature_counts`: A 2D Numpy array of dimensions `self.num_classes` $\\times$ $|V|$. The $(c,k)$-entry in this array should be the number of times that term $k$ appears in documents belonging to class $c$. Note that we're using the Bag-of-Words approach here, so if a term appears multiple times in a single document, each instance of that term should be counted.  \n",
    "\n",
    "When you think you're done, execute the following unit test, corresponding to the example starting on Slide 29 of [Lecture 24](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson24.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:46.325834Z",
     "start_time": "2018-04-03T14:45:46.314267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testVocab (__main__.TestNB) ... ok\n",
      "testClassCounts (__main__.TestNB) ... ok\n",
      "testFeatureCounts (__main__.TestNB) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict_log_score` function in the `TextNB` class to take in a single string of text and compute the associated log-score for each class. For now, you should use add-one Laplace smoothing for both the class-priors and the class-conditional probabilities.  In **Problem 3** we'll experiment with different variants of Laplace smoothing, so if you like you can read ahead now and implement the general version of Laplace smoothing from the beginning.  \n",
    "\n",
    "**Note**: For simplicity and testing purposes, do not implement an `UNK` feature.  Instead, if you encounter a term not in the vocabulary you can safely ignore it. \n",
    "\n",
    "When you think your `predict_log_score` function is working well, execute the following unit test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:47.998683Z",
     "start_time": "2018-04-03T14:45:47.990764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testLogScore (__main__.TestNB) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: testLogScore (__main__.TestNB)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\llair\\Documents\\ML\\CSCI-4622-Machine-Learning\\hmwk\\hmwk05\\tests\\tests.py\", line 69, in testLogScore\n",
      "    class_scores = self.nb.predict_log_score(self.text_train[2])\n",
      "  File \"<ipython-input-23-065937250763>\", line 72, in predict_log_score\n",
      "    for i,c in range(self.num_classes):\n",
      "TypeError: 'int' object is not iterable\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally, implement the `predict` method to take in a list or ndarray of text data, call `predict_log_score`, and return a vector of predicted labels. \n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:49.228361Z",
     "start_time": "2018-04-03T14:45:49.220931Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 2C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 3: Predicting the Sentiment of Tweets sent from Passengers to Airlines \n",
    "***\n",
    "\n",
    "In this problem you'll use the `TextNB` class you wrote in **Problem 2** to make predictions about the sentiment of tweets sent by passengers to airlines.  Execute the following cell to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:55.395606Z",
     "start_time": "2018-04-03T14:45:55.371833Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/airline_tweets.pklz','rb')\n",
    "text_train, y_train, text_valid, y_valid, text_all, y_all = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@southwestair awesome  thanks'\n",
      " '@usairways good news weve located the crew and made contact with them flight was supposed to leave 4 minutes ago usairwaysfail'\n",
      " '@usairways 45 minute delay for take off and 30 minute wait for checked bags really'\n",
      " ...,\n",
      " '@southwestair will continue to be my airline of choice @united most frustrating travel day ive experienced 5 delays and 4 gate changes'\n",
      " '@americanair would it be ok to send you a dm asking a few questions because im and been on hold so long'\n",
      " '@united this is it last time i fly unitedairlines  you screw up every trip now will be stuck in ord and miss work']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Explore the data and answer the following questions: \n",
    "\n",
    "- How many total examples are there in the training and validation sets? \n",
    "- Which binary label ($\\{0,1\\}$) corresponds to tweets with positive and negative sentiment, respectively?\n",
    "- What percentage of tweets in the training set have true positive sentiment? \n",
    "- What percentage of tweets in the validation set have true positive sentiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use your `TextNB` class to learn a Naive Bayes classifier for the airline Twitter data.  What accuracy do you achieve on the training set and what accuracy do you achieve on the test set? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Notice that if we want to make improvements in our Naive Bayes classifier, we don't really have a lot of knobs to turn aside from changing the word features that we use.  One place that we might make some gains though is in using a slightly different version of Laplace smoothing.  \n",
    "\n",
    "Recall that in add-one smoothing we add a $1$ to the numerator in both the estimation of the prior probabilities and the class-conditional likelihoods. \n",
    "\n",
    "$$\n",
    "\\hat{p}(\\textrm{Class}) = \\dfrac{\\textrm{# docs from Class}+1}{\\textrm{# total docs in training data} + |C|},\n",
    "\\quad \\quad\n",
    "\\hat{p}(\\textrm{term} \\mid \\textrm{Class}) = \\dfrac{\\textrm{# instance of term in Class}+1}{\\textrm{# total words in Class} + |V|}\n",
    "$$\n",
    "\n",
    "It turns out there's nothing sacred about adding $1$ to the numerators.  In fact, we can add any positive value $\\alpha$ that we like \n",
    "\n",
    "$$\n",
    "\\hat{p}(\\textrm{Class}) = \\dfrac{\\textrm{# docs from Class}+\\alpha}{\\textrm{# total docs in training data} + ?},\n",
    "\\quad \\quad\n",
    "\\hat{p}(\\textrm{term} \\mid \\textrm{Class}) = \\dfrac{\\textrm{# instance of term in Class}+\\alpha}{\\textrm{# total words in Class} + ?}\n",
    "$$\n",
    "\n",
    "Explain what modification must be made to the denominators so that theses estimates remain valid probabilities. Clearly justify your reasoning. \n",
    "\n",
    "Support this modification in your `TextNB` class above, if you have not already.  Make sure that your code still passes then unit tests when $\\alpha = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Write some code to perform $K$-Folds cross-validation on the entire data set (`train_all` and `y_all`) to estimate the accuracy of your NB classifier for various values of $\\alpha$ and make a plot showing your results.  \n",
    "\n",
    "To do the partitioning into folds I recommend leveraging sklearn's [StratifiedKFold]() routine.  The documentation demonstrates how it can be used.  \n",
    "\n",
    "For your plot, use at least $K=5$ folds and at least $5$ different values of $\\alpha$ between $0.1$ and $1.5$.  Which value of $\\alpha$ seems to perform the best? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 4: VC Dimension \n",
    "***\n",
    "\n",
    "**Part A**: Consider learning to classify binary labeled data with a single feature $x$.  Let $H$ be the hypothesis class described by the union of two intervals $[a,b] \\cup [c,d]$ such that $h(x)$ labels an example as positive if it's in the interval $[a,b]$ **OR** the interval $[c,d]$.  Determine the VC Dimension of $H$.  Justify your conclusion by demonstrating a shattering of a set $S$ of the appropriate size **AND** by arguing that an arbitrary set consisting of one additional point cannot be shattered by $H$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Consider learning to classify binary labeled data with two features, $x_1$ and $x_2$.  Let $H$ be the hypothesis class described by the ability to assign all points in a particular quadrant of the 2D plane to be positive or negative, with the restriction that at least one of the four quadrants must be labeled positive and at least one of the four quadrants must be labeled negative. Determine the VC Dimension of $H$. Again, justify your conclusion by demonstrating a shattering of a set $S$ of the appropriate size **AND** by arguing that an arbitrary set consisting of one additional point cannot be shattered by $H$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
