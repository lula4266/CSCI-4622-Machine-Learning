{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Regression, Regularization, and the Bias-Variance Trade-Off\n",
    "***\n",
    "\n",
    "**Name**: Lucas Laird\n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 2nd**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 1 - Getting Comfortable with MathJax/LaTeX\n",
    "***\n",
    "\n",
    "Most homeworks in this course will require you to write solutions to at least one or two math-based exercises.  This problem is designed to motivate you to learn some MathJax-LaTeX for typesetting math in Jupyter Notebooks.  [LaTeX](https://en.wikipedia.org/wiki/LaTeX) is a markup language used for typesetting mathematical formulas and documents.  [MathJax](https://en.wikipedia.org/wiki/MathJax) is a LaTeX plug-in for Markdown that brings some LaTeX functionality to Jupyter. A good tutorial on MathJax can be found in this [StackExchange post](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference/5024). \n",
    "\n",
    "Your goal is this problem is to use MathJax to reproduce a collection of formulas as accurately as possible. Note that because computing environments vary from person to person, you shouldn't worry about tiny details like matching fonts (though you should worry about normal vs italic vs bold).  Just do your best to get reasonably close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A** Reproduce the expression shown below using MathJax: \n",
    "\n",
    "<img src=\"figs/prob1A.png\" width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\hat y_i = \\hat \\beta_0+ \\hat \\beta_1 x_{i1}+\\hat \\beta_2 x_{i2}+ ...+ \\hat \\beta_p x_{ip}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B** Reproduce the expression shown below using MathJax: \n",
    "\n",
    "<img src=\"figs/prob1B.png\" width=\"180\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11}\\\\\n",
    "1&x_{21}\\\\\n",
    "1&x_{31}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C** Reproduce the expression shown below using MathJax: \n",
    "\n",
    "<img src=\"figs/prob1C.png\" width=\"250\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ p(y=1 | x) = \\frac{1}{1+exp(-x^T\\beta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 2 - Building a Data Storage Class \n",
    "***\n",
    "\n",
    "In this exercise you will get some practice constructing classes in Python.  If you are rusty on object-oriented Python, you should check out [this tutorial](https://www.digitalocean.com/community/tutorials/how-to-construct-classes-and-define-objects-in-python-3).  Our goal will be to create a class which takes in a set of labeled data, randomly splits it into training and validation sets, and then stores these sets for later use. You will also implement the ability to mean-center and standardize features, and apply these same transformations to new test data after the fact.  A starting point for RegressionData class appears below.  \n",
    "\n",
    "**Notes**: \n",
    "- This problem will be graded by unit test, so do not change function or method APIs, but feel free to create any additional variables or methods that you think will be helpful. We've given you access to versions of those unit tests down below, so you'll be able to check your work as you go.  \n",
    "- Do not use any additional functions from sklearn.  (There is an sklearn function called train_test_split that does something similar.  You'll be allowed to use it later on.  This exercise is about getting your hands dirty so you can understand how things work under the hood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionData:\n",
    "    \"\"\"\n",
    "    Class to store data for regression problems \n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, train_frac=0.8, center=False, standardize=False, random_state=1234):\n",
    "        \"\"\"\n",
    "        Creates a RegressionData instance\n",
    "\n",
    "        :param X: (n x p) ndarray of feature data \n",
    "        :param y: (n x 1) ndarray of labels/targets  \n",
    "        :param train_frac: float indicating fraction of data to train on \n",
    "        :param center: bool indicating whether to mean-center the features \n",
    "        :param standardize: bool indicating whether to mean_center and standardize \n",
    "        :param random_state: integer seed for random number generators\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set random seed (for testing purposes, don't change this line) \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # These should probably be set, eventually\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_valid = None\n",
    "        self.y_valid = None\n",
    "        \n",
    "        # Perform train-validation split \n",
    "        self.train_valid_split(X, y, train_frac)\n",
    "            \n",
    "        # standardize and/or center the data if requested\n",
    "        self.transform_train_valid(center, standardize)\n",
    "        \n",
    "    def train_valid_split(self, X, y, train_frac):\n",
    "        \"\"\"\n",
    "        Randomly splits the data into training and validation sets \n",
    "\n",
    "        :param X: (n x p) ndarray of feature data \n",
    "        :param y: (n x 1) ndarray of labels/targets  \n",
    "        :param train_frac: float indicating fraction of data to train on \n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        ranInd = np.random.permutation(n)\n",
    "        numTrain = round(n*train_frac)\n",
    "        trainInd = ranInd[0:numTrain]\n",
    "        testInd = ranInd[numTrain:]\n",
    "        self.X_train = X[trainInd]\n",
    "        self.y_train = y[trainInd]\n",
    "        self.X_valid = X[testInd]\n",
    "        self.y_valid = y[testInd]\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def transform_train_valid(self, center, standardize):\n",
    "        \"\"\"\n",
    "        Standardizes and/or centers train and validation sets, if requested\n",
    "\n",
    "        :param center: bool indicating whether to mean-center the features \n",
    "        :param standardize: bool indicating whether to mean_center and standardize \n",
    "        \"\"\" \n",
    "        Xm = np.mean(self.X_train, axis = 0)\n",
    "        Xs = np.std(self.X_train, axis = 0)\n",
    "        self.mean = Xm\n",
    "        self.std = Xs\n",
    "        if(center):\n",
    "            for i,c in enumerate(self.X_train.T):\n",
    "                temp = c-Xm[i]\n",
    "                self.X_train[:,i] = temp\n",
    "            for i,c in enumerate(self.X_valid.T):\n",
    "                temp = c-Xm[i]\n",
    "                self.X_valid[:,i] = temp\n",
    "        if(standardize):\n",
    "            for i,c in enumerate(self.X_train.T):\n",
    "                temp = c/Xs[i]\n",
    "                self.X_train[:,i] = temp\n",
    "            for i,c in enumerate(self.X_valid.T):\n",
    "                temp = c/Xs[i]\n",
    "                self.X_valid[:,i] = temp\n",
    "            \n",
    "        return\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies transformations performed on training set to \n",
    "        previously unseen data x \n",
    "\n",
    "        :param X: (m x p) ndarray of feature data \n",
    "        \"\"\"\n",
    "        Xt = X.copy()\n",
    "        for i,c in enumerate(Xt.T):\n",
    "            temp = (c-self.mean[i])/self.std[i]\n",
    "            Xt[:,i] = temp\n",
    "        return Xt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the method `train_valid_split` to randomly initialize the train and validation data according to the split proportion indicated by `train_frac`. When you think you're done, execute the unit tests at the end of this problem.  If you're successful, you should pass the first 2 tests.  Don't move on until you've passed them! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the method `transform_train_valid` function to standardize and/or center the features depending on the values of `center` and `standardize`.  Remember that if both flags are set, each column in the **training data** should be shifted to have mean zero, and then scaled to have standard deviation of 1. Each column of the validation data should then undergo the same shift and scale **based on the training data**.  When you think you're done, execute the unit tests again. If you're successful, you should pass four more unit tests.  Don't move on until you do! \n",
    "\n",
    "**Notes**: \n",
    "\n",
    "- How you implement this method might affect **Part C**.  It's a good idea to read **Part C** now so that you can plan ahead. \n",
    "- It's rare in a regression setting that you would ever standardize a feature without centering it first, so we will not be testing this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Suppose that later on in your pipeline you collect a new set of data that you would like to test your model on.  Since your model was probably constructed using the transformed features of your training set, you have to apply that same transformation to your new data.  Complete the `transform` function above so that it takes in an ndarray of newly acquired data and returns the appropriately transformed version. When you think you're done, execute the unit tests one more time.  If you're successful, you should pass the final test, at which point you will be done with this problem forever (sorta).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.016s\n",
      "\n",
      "OK\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.000s\n",
      "\n",
      "OK\n",
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3.85714285714 2.58725289661\n",
      "Test 13.8571428571 2.58725289661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to run unit tests \n",
    "%run -i tests/tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 3 - Solving Ridge Regression\n",
    "***\n",
    "\n",
    "In this problem you'll derive a closed form solution for single-feature Ridge Regression and use it to interpret the behavior for different values of the regularization parameter $\\lambda$. Recall that for a model of the form $Y = \\beta_0 + \\beta_1 X + \\epsilon$ and a training set $\\{(x_i, y_i)\\}_{i=1}^n$, the parameter estimates are obtained by minimizing the penalized RSS: \n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\textrm{argmin}} ~\\textrm{RSS}_\\lambda =  \\underset{\\boldsymbol{\\beta}}{\\textrm{argmin}} \\sum_{i=1}^n \\left( \\beta_0 + \\beta_1 x_i - y_i\\right)^2 + \\lambda \\beta_1^2\n",
    "$$\n",
    "\n",
    "In Calc 1 you saw that you can find the value of the parameter that minimizes a function by taking a derivative with respect to that parameter, setting it to 0, and solving. The problem here is that we have **two** parameters we need to minimize over. It turns out, for a certain class of problems, we can solve for two parameters in a similar way.  For two parameters we can take _partial_ derivatives with respect to each parameter, set equal to zero, and solve simultaneously for the optimal parameters.  That is, we want to solve the following two equations for $\\beta_0$ and $\\beta_1$: \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial~\\textrm{RSS}_\\lambda}{\\partial \\beta_0} = 0 \n",
    "\\quad \\textrm{and} \\quad\n",
    "\\dfrac{\\partial~\\textrm{RSS}_\\lambda}{\\partial \\beta_1} = 0 \n",
    "$$\n",
    "\n",
    "Simple enough, right?  Kinda.  It turns out that even this is a bit tricky, unless you modify your data in a particular way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose that we mean-center our feature $x$ by replacing each $x_i$ by $\\hat{x}_i = x_i - \\bar{x}$. Show that in this case, the Ridge Regression estimate of the bias is $\\hat{\\beta}_0 = \\bar{y}$ (the sample mean of the response $y$) by taking the partial derivative of $\\textrm{RSS}_\\lambda$ with respect to $\\beta_0$, setting it to zero, and solving for $\\beta_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial RSS_\\lambda}{\\partial \\beta_0} = 2\\sum_1^n(\\beta_0+\\beta_1 (x_i-\\bar{x}) - y_i) + 0 = 0 \\\\\n",
    "= 2n\\beta_0 + 2\\sum \\beta_1 (x_i-\\bar{x}) - 2\\sum y_i = 0 \\\\\n",
    "\\sum y_i = n\\beta_0 + \\beta_1 (\\sum x_i - n\\bar{x}) \\\\\n",
    "\\sum y_i = n\\beta_0 + \\beta_1(0) \\\\\n",
    "\\frac{\\sum y_i}{n} = \\beta_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Next, solve for the Ridge Regression estimate of the slope, $\\hat{\\beta}_1$, by taking the partial derivative of $\\textrm{RSS}_\\lambda$ with respect to $\\beta_1$, setting it to zero, and solving for $\\beta_1$ as a function of the data and of $\\lambda$.  (**Hint**: You'll want to use the value of $\\hat{\\beta}_0$ you found in **Part A**). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial RSS_\\lambda}{\\partial \\beta_1} = 2\\sum((x_i-\\bar{x})(\\beta_0+\\beta_1 (x_i-\\bar{x}) - y_i)) + 2\\lambda \\beta_1 \\\\\n",
    "= \\sum((x_i-\\bar{x})(\\bar{y}-y_i+\\beta_1 (x_i-\\bar{x}))) + \\lambda \\beta_1 \\\\\n",
    "= \\sum((x_i-\\bar{x})(\\bar{y}-y_i) + \\beta_1 \\sum(x_i-\\bar{x})^2 + \\lambda \\beta_1\\\\\n",
    "-\\sum((x_i-\\bar{x})(\\bar{y}-y_i) = \\beta_1(\\sum(x_i-\\bar{x})^2 + \\lambda) \\\\\n",
    "\\beta_1 = \\frac{-\\sum((x_i-\\bar{x})(\\bar{y}-y_i)}{(\\sum(x_i-\\bar{x})^2 + \\lambda)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use the closed-form solution you found in **Part B** to _briefly_ describe the effect of the regularization parameter $\\lambda$ on the slope parameter as $\\lambda \\rightarrow 0$ and $\\lambda \\rightarrow \\infty$. Does this jive with the discussion of regularization from class? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As $\\lambda \\rightarrow 0$, $\\beta_1$ gets closer to $\\frac{-\\bar{y}}{\\bar{x}}$. As $\\lambda \\rightarrow \\infty$, $\\beta_1$ goes to 0. This jives with the discussion since as $\\lambda$ increases, the regression is \"punished\" more heavily for having a non-zero $\\beta_1$ value and will try to make it lower and lower. In contrast if $\\lambda = 0$ it is just normal linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Verify that the solution obtained in **Parts A** and **B** are correct by testing it on some simulated data and comparing the results to those obtained by sklearn's [Ridge Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) routine.  We've given you some data below, but feel free to mess with the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (200,) not aligned: 1 (dim 0) != 200 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ad456e8a8def>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mxdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mydiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mydiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,) and (200,) not aligned: 1 (dim 0) != 200 (dim 0)"
     ]
    }
   ],
   "source": [
    "lam = 2.0 \n",
    "X = np.random.uniform(size=(200,1))\n",
    "y = 2 + 3 * X[:,0] + 0.5*np.random.randn(200)\n",
    "\n",
    "# Load the data into our data structure and center it \n",
    "cdata = RegressionData(X, y, center=True)\n",
    "\n",
    "# Compute beta_0 and beta_1 using the formulas you derived above\n",
    "cdata.\n",
    "print(b0, b1)\n",
    "\n",
    "# Compute beta_0 and beta_1 by calling Ridge()\n",
    "from sklearn.linear_model import Ridge \n",
    "model = Ridge(alpha = lam)\n",
    "model.fit(cdata.X_train,cdata.y_train)\n",
    "b0 = model.intercept_\n",
    "b1 = model.coef_[0]\n",
    "print(b0,b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: We claimed in the class that if you built a regularized regression model based on centered features, the model would make the same prediction as an equivalent model built with un-centered features. Verify this fact by \n",
    "\n",
    "- Predicting the response for your centered Validation set using the model you computed on your own.  \n",
    "- Predicting the response for the equivalent un-centered Validation set using sklearn's Ridge model trained on the un-centered features. \n",
    "- Showing that the predictions are the same for a reasonable subset of the Validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 4 - Polynomial Regression  and the Bias-Variance Trade-Off\n",
    "***\n",
    "\n",
    "In this problem you will use polynomial regression to explore the Bias-Variance Trade-Off. Assume that our data comes from a model of the form \n",
    "\n",
    "$$Y = f(X) + \\epsilon ~~\\textrm{ where }~~ \\epsilon \\sim N(0,\\sigma^2)$$ \n",
    "\n",
    "For our experiments we'll use $f(x) = \\sin(\\pi x)$. The following functions can be used to generate data from this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def true_f(x):\n",
    "    \"\"\"\n",
    "    Returns sin(pi*x) for array of x values\n",
    "    \n",
    "    :@param x: ndarray of feature values\n",
    "    \"\"\"\n",
    "    return np.sin(np.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Generate a sample of size $n=20$ for $x$-values chosen from a uniform distribution between $0$ and $1$.  Make a scatter plot of the data overlayed with the curve of the true function $f(x) \\sin(\\pi x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27f2d2aad68>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEVlJREFUeJzt3V9o3ed9x/H3VxZZ0Zq2I1JhxLaU\nUQdqwiCryFp20ZZm4PTCvumKjcxWCBFNSHfR0pHhEUqKL9YyOgZOV20t3erTpm4vWlNSUsgSOkpd\nopA2WxIMrms7IoUoaZYbkaaJv7v4HVWyIun8jn3O7xw95/0Ccc7vOQ9HXz+SPufx78/zi8xEklSW\nsUEXIEnqPcNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVKDxQX3jycnJnJmZGdS3\nl6Qd6cknn3wpM6c69RtYuM/MzLC4uDioby9JO1JEXKzTz90yklQgw12SCmS4S1KBDHdJKlDHcI+I\nr0XEixHxv1u8HhHxLxFxLiKejog/632ZkqRu1Jm5fx04sM3rdwD72l/zwJevvSxpiLRaMDMDY2PV\nY6s16IqkjjqGe2b+GPjNNl0OAf+ZlTPAuyLij3tVoDRQrRbMz8PFi5BZPc7PX13A+yGhBvVin/uN\nwPPrtpfabdLOd+wYrKxc2bayUrV3o5cfElINvQj32KRt0xuzRsR8RCxGxOLy8nIPvrXUZ5cudde+\nlV59SEg19SLcl4A967Z3Ay9s1jEzFzJzNjNnp6Y6Xj0r9cbq7pAIGB+vHuvuFtm7t7v2rfTqQ0Kq\nqRfhfhr46/ZZM+8HXs3MX/fgfaVr02rB5CQcPVrtBgF4883qse5ukePHYWLiyraJiaq9G1fzIeE+\nel2LzNz2C/gW8Gvgd1Sz9DuBTwKfbL8ewAngl8D/ALOd3jMzed/73pdS35w8mTkxkVnt4d76a3q6\n3ntNT2dGVI8nT/amnomJrd+r2/4aGcBi1sjYqPo2b3Z2Nl04TH0zM7M2W99OBFy+3PdygGrmfexY\ntStm795q9j83t3nfreqfnoYLF/pZpYZcRDyZmbMd+xnuKtLYWDXf7WRYw3Kr+pv8MNJQqhvuLj+g\nMtU54Hk1+86b0qsDuRpZhrvKtNmBUKhmvlDN2BcWtt4tMmi9OpCrkWW4q0xzc1V4T09XgT49DSdP\nVrs0MqtdMcMa7LB5/cP8YaSh4z53SdpB3OcuSSPMcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkF\nMtwlqUCGuyQVyHBXs7wBhdSI8UEXoBGyepPo1XuJrt4NCVwzReoxZ+5qjjeJlhpjuKs53iRaaozh\nruZ4A4rh5zGRYhjuao43oBhuq8dELl6s1rxfPSZiwO9Ihrua4w0ohpvHRIrizTokVbwp947gzTok\ndcdjIkUx3CVVPCZSFMNdUsVjIkXxClVJa+bmDPNCOHOXpAIZ7pJUIMNdV8+rGUeXP/uhZ7jr6ng1\n4+ja7Gd/9ChMTvrzHyJexKSrMzNT/VFvND0NFy40XY2atNXPHqpTJz3Dpq+8iEn95QqPo2u7n7HL\nFQwNw11Xx6sZR1enn7Ef8EOhVrhHxIGIOBsR5yLivk1e3xsRj0XEUxHxdER8tPelaqDuuQfGx6uL\nW8bH4T3v8WrGUbXZlazr+QE/FDqGe0TsAk4AdwD7gSMRsX9Dt38ATmXmrcBh4MFeF6oBuuce+PKX\n4c03q+0334RHH4UPfMCrGUfR6pWsN9zw1tf8gB8adWbutwHnMvN8Zr4OPAQc2tAngXe0n78TeKF3\nJWrgFhY2b3/88erg6eXL1aPBPjrm5uCll+DkST/gh1Sd5QduBJ5ft70E/PmGPp8DfhQRnwL+ELh9\nszeKiHlgHmCv/3XbOVZn7HXbNTpcrmBo1Zm5xyZtG8+fPAJ8PTN3Ax8FvhERb3nvzFzIzNnMnJ2a\nmuq+Wg3Grl3dtUsauDrhvgTsWbe9m7fudrkTOAWQmT8F3gZM9qJADYH5+e7aJQ1cnXB/AtgXETdF\nxHVUB0xPb+hzCfgIQES8lyrcl3tZqAbowQfh7rvXZuq7dlXbD3rcXB24TMHA1LpCtX1q4z8Du4Cv\nZebxiHgAWMzM0+2zZ/4NeDvVLpu/y8wfbfeeXqEqFW51mYL192X1CtZrVvcKVZcfkNQfLlHRFy4/\nIGmwXKJioAx3Sf3hEhUDZbhL6g9vuD1Qhruk/vCG2wPlDbIl9Y9XsA6MM3dJKpDhLkkFMtwlqUCG\nuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhL\nUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUC1wj0i\nDkTE2Yg4FxH3bdHn4xHxbEQ8ExHf7G2ZkqRujHfqEBG7gBPAXwJLwBMRcTozn13XZx/w98BfZOYr\nEfHufhUsSeqszsz9NuBcZp7PzNeBh4BDG/rcBZzIzFcAMvPF3pYpSepGnXC/EXh+3fZSu229m4Gb\nI+InEXEmIg70qkBJUvfqhHts0pYbtseBfcCHgCPAv0fEu97yRhHzEbEYEYvLy8vd1jq6Wi2YmYGx\nseqx1Rp0RZKGXJ1wXwL2rNveDbywSZ/vZ+bvMvNXwFmqsL9CZi5k5mxmzk5NTV1tzaOl1YL5ebh4\nETKrx/l5A17StuqE+xPAvoi4KSKuAw4Dpzf0+R7wYYCImKTaTXO+l4WOrGPHYGXlyraVlapdkrbQ\nMdwz8w3gXuAR4DngVGY+ExEPRMTBdrdHgJcj4lngMeCzmflyv4oeKZcuddcuSUBkbtx93ozZ2dlc\nXFwcyPfeUWZmql0xG01Pw4ULTVcjacAi4snMnO3UzytUh93x4zAxcWXbxETVLklbMNyH3dwcLCxU\nM/WI6nFhoWqXpC0Y7sNo46mPUO2CuXy5ejTYJXXQcfkBNWz11MfVM2RWT30EQ11Sbc7ch42nPkrq\nAcN92Hjqo6QeMNyHzd693bVL0iYM92HjqY+SesBwHzae+iipBzxbZhjNzRnmkq6JM3dJKpDhLkkF\nMtwlqUCGuyQVyHCXpAIZ7oPgPVEl9ZmnQjbNhcEkNcCZe9NcGExSAwz3prkwmKQGGO5Nc2EwSQ0w\n3JvmwmCSGmC4N82FwSQ1wLNlBsGFwST1mTN3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIK\nZLhLUoEMd0kqkOEuSQWqFe4RcSAizkbEuYi4b5t+H4uIjIjZ3pUoSepWx3CPiF3ACeAOYD9wJCL2\nb9LveuBvgZ/1ukhJUnfqzNxvA85l5vnMfB14CDi0Sb/PA18AXuthfZKkq1An3G8Enl+3vdRu+72I\nuBXYk5k/2O6NImI+IhYjYnF5ebnrYiVJ9dQJ99ikLX//YsQY8CXgM53eKDMXMnM2M2enpqbqVylJ\n6kqdcF8C9qzb3g28sG77euAW4PGIuAC8HzjtQVVJGpw64f4EsC8iboqI64DDwOnVFzPz1cyczMyZ\nzJwBzgAHM3OxLxVLkjrqGO6Z+QZwL/AI8BxwKjOfiYgHIuJgvwuUJHWv1m32MvNh4OENbfdv0fdD\n116WJOlaeIWqJBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJU\nIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy\n3CWpKa0WzMzA2Fj12Gr17VuN9+2dJUlrWi2Yn4eVlWr74sVqG2Buruffzpm7JDXh2LG1YF+1slK1\n94HhLklNuHSpu/ZrZLhLUhP27u2u/RoZ7pLUhOPHYWLiyraJiaq9Dwx3SWrC3BwsLMD0NERUjwsL\nfTmYCp4tI0nNmZvrW5hv5MxdkgpUK9wj4kBEnI2IcxFx3yavfzoino2IpyPi0YiY7n2pkqS6OoZ7\nROwCTgB3APuBIxGxf0O3p4DZzPxT4LvAF3pdqCSpvjoz99uAc5l5PjNfBx4CDq3vkJmPZebq2fln\ngN29LVOS1I064X4j8Py67aV221buBH54LUVJkq5NnbNlYpO23LRjxFFgFvjgFq/PA/MAe/t04r4k\nqd7MfQnYs257N/DCxk4RcTtwDDiYmb/d7I0ycyEzZzNzdmpq6mrqlSTVUCfcnwD2RcRNEXEdcBg4\nvb5DRNwKfIUq2F/sfZmSpG50DPfMfAO4F3gEeA44lZnPRMQDEXGw3e2LwNuB70TEzyPi9BZvJ0lq\nQK0rVDPzYeDhDW33r3t+e4/rkiRdA69QlaQCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUqM9xbLZiZ\ngbGx6rHVGnRFktSo8u7E1GrB/DystBepvHix2obG7oAiSYNW3sz92LG1YF+1slK1S9KIKC/cL13q\nrl2SClReuG+1lLBLDEsaIeWF+/HjMDFxZdvERNUuSSOivHCfm4OFBZiehojqcWHBg6mSRkp5Z8tA\nFeSGuaQRVt7MXZJkuEtSiQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoF2Vrh7\nhyVJqmXnrC3jHZYkqbadM3P3DkuSVNvOCXfvsCRJte2ccPcOS5JU284Jd++wJEm17Zxw9w5LklTb\nzjlbBrzDkiTVtHNm7pKk2mqFe0QciIizEXEuIu7b5PU/iIhvt1//WUTM9LpQSVJ9HcM9InYBJ4A7\ngP3AkYjYv6HbncArmfke4EvAP/a6UElSfXVm7rcB5zLzfGa+DjwEHNrQ5xDwH+3n3wU+EhHRuzIl\nSd2oE+43As+v215qt23aJzPfAF4Fbtj4RhExHxGLEbG4vLx8dRVLkjqqc7bMZjPwvIo+ZOYCsAAQ\nEcsRcbHG958EXqrRb1Q4HmscizWOxZrSx2K6Tqc64b4E7Fm3vRt4YYs+SxExDrwT+M12b5qZU3UK\njIjFzJyt03cUOB5rHIs1jsUax6JSZ7fME8C+iLgpIq4DDgOnN/Q5DfxN+/nHgP/KzLfM3CVJzeg4\nc8/MNyLiXuARYBfwtcx8JiIeABYz8zTwVeAbEXGOasZ+uJ9FS5K2V+sK1cx8GHh4Q9v9656/BvxV\nb0v7vYU+ve9O5XiscSzWOBZrHAsg3HsiSeVx+QFJKtDQhLtLHKypMRafjohnI+LpiHg0ImqdGrVT\ndRqPdf0+FhEZEcWeKVFnLCLi4+3fj2ci4ptN19iUGn8neyPisYh4qv238tFB1DkwmTnwL6oDtb8E\n/gS4DvgFsH9Dn3uAf20/Pwx8e9B1D3AsPgxMtJ/fXepY1B2Pdr/rgR8DZ4DZQdc9wN+NfcBTwB+1\nt9896LoHOBYLwN3t5/uBC4Ouu8mvYZm5u8TBmo5jkZmPZebqDWXPUF17UKo6vxsAnwe+ALzWZHEN\nqzMWdwEnMvMVgMx8seEam1JnLBJ4R/v5O3nr9TlFG5Zw79kSBwWoMxbr3Qn8sK8VDVbH8YiIW4E9\nmfmDJgsbgDq/GzcDN0fETyLiTEQcaKy6ZtUZi88BRyNiiepsv081U9pwGJabdfRsiYMC1P53RsRR\nYBb4YF8rGqxtxyMixqhWIv1EUwUNUJ3fjXGqXTMfovof3X9HxC2Z+X99rq1pdcbiCPD1zPyniPgA\n1bU4t2Tm5f6XN3jDMnPvZokD6i5xsEPVGQsi4nbgGHAwM3/bUG2D0Gk8rgduAR6PiAvA+4HThR5U\nrft38v3M/F1m/go4SxX2pakzFncCpwAy86fA26jWnRkJwxLuLnGwpuNYtHdDfIUq2Evdp7pq2/HI\nzFczczIzZzJzhuoYxMHMXBxMuX1V5+/ke1QH3ImISardNOcbrbIZdcbiEvARgIh4L1W4j8xytEMR\n7u196KtLHDwHnMr2EgcRcbDd7avADe0lDj4NbHlK3E5Wcyy+CLwd+E5E/DwiNv5SF6PmeIyEmmPx\nCPByRDwLPAZ8NjNfHkzF/VNzLD4D3BURvwC+BXyi0AnhprxCVZIKNBQzd0lSbxnuklQgw12SCmS4\nS1KBDHdJKpDhLkkFMtwlqUCGuyQV6P8BscOaKdeP4EkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27f2d334630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.uniform(size= (20,1))\n",
    "y = true_f(x)\n",
    "plt.plot(x,y, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Next we need to create a function that can fit a polynomial model to training data and make predictions for unseen data.  Complete the function `poly_predict` below to accomplish this.  Note that there are many ways to do this in Python.  Later this week we'll look into doing this with Scikit-Learn.  You're free to implement it using Scikit-Learn, but you might want to look into Numpy's polynomial fitting functions, [polyfit](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html) and [polyval](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyval.html).\n",
    "\n",
    "Demonstrate that your function is working by using it to generate a plot (similar to those shown in lecture) that include the true function $f(x)$, a scatter plot of your training data, and the curve representing your fitted model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly_predict(x_train, y_train, x_valid, deg):\n",
    "    \"\"\"\n",
    "    Function to train polynomial regression model on training data\n",
    "    and then return a vector of predictions on validation data\n",
    "    \n",
    "    :@param x_train: vector of training features\n",
    "    :@param y_train: vector of training responses\n",
    "    :@param x_valid: vector of validation features to make predictions with\n",
    "    :@param deg: degree of the polynomial model \n",
    "    \"\"\"\n",
    "    prediction = np.zeros_like(x_valid) #TODO\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Our goal now will be to make a plot of the decomposition of the expected validation MSE into it's constituent parts.  Recall that we showed in lecture that the expected test MSE can be written as \n",
    "\n",
    "$$\n",
    "\\textrm{E}\\left[\\left(y_0 - \\hat{f}(x_0) \\right)^2\\right] = \\left[~f(x_0) - \\textrm{E}[~\\hat{f}(x_0)~] \\right]^2\n",
    "+ \\textrm{E}\\left[ ~ \\textrm{E}[~\\hat{f}(x_0)]-\\hat{f}(x_0)~\\right]^2 + \\textrm{Var}(\\epsilon)\n",
    "= \\left[\\textrm{Bias}(~\\hat{f}(x_0)\\right]^2 + \\textrm{Var}(~\\hat{f}(x_0)~) + \\textrm{Var}(\\epsilon)\n",
    "$$\n",
    "\n",
    "where $x_0$ represents unseen validation data. We're going to run simulations to estimate $\\left[\\textrm{Bias}(~\\hat{f}(x_0)\\right]^2$ and $\\textrm{Var}(~\\hat{f}(x_0)~)$ for different polynomial models and then plot them against estimates of the true validation MSE. The function `bias_variance_study` below will help you do this.  You just need to fill in a few key parts.  At the end of the day, you should have a plot that looks like this for certain choices of the simulation parameters. \n",
    "\n",
    "The $\\textrm{Bias}^2$ term is given by \n",
    "\n",
    "$$\n",
    "\\left[\\textrm{Bias}(~\\hat{f}(x_0)\\right]^2 = \\left[~f(x_0) - \\textrm{E}[~\\hat{f}(x_0)~] \\right]^2\n",
    "$$\n",
    "\n",
    "The first term inside the square is simply the true function $f$ evaluated on the validation data.  The second term inside the square, $\\textrm{E}[~\\hat{f}(x_0)~]$, is the expected value of all estimated models evaluated on the validation data.  We can estimate this by sampling many many training sets, fitting models, evaluating them on many validation sets, and then taking the average. Complete the `squared_bias` function below to do this computation.  \n",
    "\n",
    "The next step is to estimate the $\\textrm{Variance}$.  Let's unpack it \n",
    "\n",
    "$$\n",
    "\\textrm{E}\\left[ ~ \\hat{f}(x_0) - \\textrm{E}[~\\hat{f}(x_0)]~\\right]^2\n",
    "$$\n",
    "\n",
    "Note that $\\textrm{E}[~\\hat{f}(x_0)]$ is the `mean_model` we got from the `squared_bias` function. The other term, $\\hat{f}(x_0)$, is a model estimated on a random training set, and then evaluated on the validation set.  Since we're wrapping this whole thing in an expectation, we're going to sample many many training sets, estimate $\\hat{f}(x_0)$, and then compute the mean squared deviation between these and the `mean_model`.  You will add code to the function below to accomplish this. \n",
    "\n",
    "We'll also estimate the validation MSE so we can compare this to the bias-variance decomposition, but this portion of the code is completed for you. \n",
    "\n",
    "Note that there are only three lines in the code that you need to modify (though, feel free to be more verbose if you like), each marked with a `#TODO`.  When everything is done, running the function call given below should produce a plot similar to [this](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/hmwk/hmwk01/figs/bias_var_plot.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variance_study(num_data, num_sims, train_frac=0.8, noiseSTD=0.4, max_deg=10, random_state=1234):\n",
    "    '''\n",
    "    Function to run simulations and estimate the squared-bias, variance, and \n",
    "    validation error of polynomial regression models. \n",
    "    \n",
    "    :@param num_data: number of points in simulated data set \n",
    "    :@param num_sims: number of simulations to run \n",
    "    :@param train_frac: fraction of total data in training set\n",
    "    :@param noiseSTD: standard deviation of noise in data \n",
    "    :@param max_deg: largest degree polynomial to analyze\n",
    "    :@param random_state: seed for random state, for reproducibility\n",
    "    '''\n",
    "    # Set random seed \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # array of polynomial degrees\n",
    "    degrees = range(1,max_deg+1)\n",
    "    \n",
    "    # train/valid split sizes \n",
    "    nTrain = int(np.ceil(train_frac * num_data))\n",
    "    nValid = num_data - nTrain\n",
    "    \n",
    "    # Generate grid of features and shuffle them \n",
    "    xGrid = np.linspace(-1,1,num_data)\n",
    "    np.random.shuffle(xGrid)\n",
    "    \n",
    "    # Spit into training and validation sets \n",
    "    xTrain = xGrid[:nTrain]\n",
    "    xValid = xGrid[nTrain:]\n",
    "    \n",
    "    # Dictionaries for predictions and MSE measurements\n",
    "    # Keys are polynomial degree. Each prediction is column \n",
    "    # of nValid x num_sims array. Each mse is entry in vector\n",
    "    # of length num_sims \n",
    "    yHatValid = {deg: np.zeros((nValid, num_sims)) for deg in degrees}\n",
    "    msesValid = {deg: np.zeros(num_sims) for deg in degrees}\n",
    "    \n",
    "    # Loop over num_sims simulated data sets\n",
    "    for tsID in range(num_sims):\n",
    "        \n",
    "        # Generate training and validation responses \n",
    "        yTrain = true_f(xTrain) + noiseSTD*np.random.randn(nTrain)\n",
    "        yValid = true_f(xValid) + noiseSTD*np.random.randn(nValid)\n",
    "        \n",
    "        # Loop over polynomial degree.  Use function from part B \n",
    "        # to fit to training set and predict on validation set.\n",
    "        # Store predictions those predictions in yHatValid[deg]\n",
    "        for deg in degrees:\n",
    "            yHatValid[deg][:,tsID] = np.zeros_like(xValid) #TODO\n",
    "            msesValid[deg][tsID] = np.mean((yValid-yHatValid[deg][:,tsID])**2)\n",
    "    \n",
    "    # Loop over each polynomial degree and compute squared-bias, variance, \n",
    "    # and mean MSE on validation set.  \n",
    "    sqBias, variance, mseValid = np.zeros(max_deg), np.zeros(max_deg), np.zeros(max_deg) \n",
    "    for ii, deg in enumerate(degrees):\n",
    "        sqBias[ii] = 0.0 #TODO\n",
    "        variance[ii] = 0.0 #TODO \n",
    "        mseValid[ii] = np.mean(msesValid[deg])\n",
    "        \n",
    "    # Plot squared bias, variance, and validation MSE \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "    ax.plot(degrees, sqBias, color=\"steelblue\", lw=3, label=\"Bias**2\")\n",
    "    ax.plot(degrees, variance, color=\"#a76c6e\", lw=3, label=\"Variance\")\n",
    "    ax.plot(degrees, mseValid, color=\"black\", lw=3, label=\"Valid MSE\")\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.set_xlabel(\"Model Complexity (Poly Degree)\", fontsize=16)\n",
    "    ax.set_ylabel(\"Error\", fontsize=16)\n",
    "    ax.legend(loc=\"upper center\", fontsize=12)\n",
    "    \n",
    "    # Return arrays of squared-Bias, variance, and MSE \n",
    "    return sqBias, variance, mseValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqBias, variance, mseValid = bias_variance_study(30, 100, train_frac=0.80, random_state=1241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: When everything is working, increase the size of the simulated data sets and the number of simulation runs and compare the difference between the validation MSE and the sum of the squared-Bias the Variance for each polynomial degree.  Try this for several values of the standard deviation of the model noise.  What do you notice?  How can you explain this result using the notions of Bias-Variance and Reducible and Irreducible error discussed in class?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
