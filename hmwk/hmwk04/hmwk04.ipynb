{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Neural Networks, SGD, and Back Propagation \n",
    "***\n",
    "\n",
    "**Name**: \n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday March 23rd**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:09.992631Z",
     "start_time": "2018-03-13T09:10:08.906587Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 1 - Single-Layer and Multilayer Perceptron Learning \n",
    "***\n",
    "\n",
    "**Part A**: Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize indicator activation functions.  For each of the following concepts, state whether the concept can be learned by a single-layer perceptron.  **Briefly** justify your response: \n",
    "\n",
    "i. $~ \\texttt{ NOT } x_1$ \n",
    "\n",
    "ii. $~~x_1 \\texttt{ NOR } x_2$ \n",
    "\n",
    "iii. $~~x_1 \\texttt{ XNOR } x_2$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "i. Yes this can be learned by a single-layer perceptron. The data is linearly separable.  \n",
    "ii. Yes this can be learned by a single-layer perceptron. The OR function is linearly separable and solvable so the solution vector for NOR would simply be 180 degrees from the solution line for OR.  \n",
    "iii. XNOR cannot be learned by a single-layer perceptron since XOR cannot be learned. This is because the points are not linearly separable since the solutions are on opposite ends of the unit rectangle from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with indicator activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. Describe your architecture and state your weight matrices and bias vectors in Markdown below. Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it correctly produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "An architecture would be the combination of 3 operations $(x_1 AND x_2) OR (x_1 NOR x_2)$. Therefore a multi-layer network with the combination of the architectures of those operations will work. The weight matrix $W^1$ is \n",
    "$ \\begin{bmatrix}\n",
    "-1 && 1 \\\\\n",
    "-1 && 1 \\\\ \n",
    "\\end{bmatrix} $  and the bias vector $b^1$ is $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. The weight matrix $W^2$ is $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and the bias vector $b^2$ is $\\begin{bmatrix} 0 \\end{bmatrix}$. Finally, all the activation functions on every node is the simple threshold at 0.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]  Output  [[1]]\n",
      "[0, 1]  Output  [[0]]\n",
      "[1, 0]  Output  [[0]]\n",
      "[0, 0]  Output  [[1]]\n"
     ]
    }
   ],
   "source": [
    "w1 = np.array([[-1,1],[-1,1]])\n",
    "b1 = np.array([[1],[-1]])\n",
    "w2 = np.array([[1],[1]])\n",
    "b2 = 0\n",
    "inputVec = [[1,1],[0,1],[1,0],[0,0]]\n",
    "for x in inputVec:\n",
    "    a1 = np.matmul(np.transpose(w1),np.array([[x[0]],[x[1]]]))+b1\n",
    "    for i,a in enumerate(a1):\n",
    "        if(a <= 0):\n",
    "            a1[i] = 0\n",
    "        else:\n",
    "            a1[i] = 1\n",
    "    a2 = np.matmul(np.transpose(w2),a1) + b2\n",
    "    for i,a in enumerate(a2):\n",
    "        if(a <= 0):\n",
    "            a2[i] = 0\n",
    "        else:\n",
    "            a2[i] = 1\n",
    "    print(x,' Output ', a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2 - Back Propagation and Deep Networks\n",
    "***\n",
    "\n",
    "In this problem you'll gain some intuition about why training deep neural networks can be very time consuming.  Consider training the chain-like neural network seen below: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "C(y, a^4) = \\frac{1}{2}(y - a^4)^2  \n",
    "$$\n",
    "\n",
    "where $a^4$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose each of the weights is initialized to $W^k = 1.0$ and each bias is initialized to $b^k = -0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On the first layer we get $1*0.5 + -0.5 = 0$ the activation is therefore $0.5$. Now feeding this forward into the second layer we get $1*0.5 + -0.5 = 0$ with the same activation giving $0.5$. Finally feeding forward, since all of the weights and biases are the same, we will get the same result, giving us the final answer $0.5$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Compute the value of $\\delta^4$ associated with the given training example. Show all work.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\delta^4 = \\frac{\\partial C}{\\partial z^4} = \\frac{\\partial C}{\\partial a^4}\\frac{\\partial a^4}{\\partial z^4} = -(y-a^4)(sigm(z^4)(1-sigm(z^4)) = -(0-0.5)(sigm(0)(1-sigm(0))) = 0.5*0.5*0.5 = 0.125  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use Back-Propagation to compute the weight and bias derivatives $\\partial C / \\partial W^k$ and $\\partial C / \\partial b^k$ for $k=1, 2, 3$.  Show all work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\frac{\\partial C}{\\partial W^k} = \\delta^{k+1} a^k$ and $\\frac{\\partial C}{\\partial b^k} = \\delta^{k+1}$  \n",
    "We therefore need to find $\\delta^3$ and $\\delta^2$ to find the partials we care about.  \n",
    "Since each layer has a single node, $\\delta^3 = \\delta^4W^3(sigm(z^3)(1-sigm(z^3)) = 0.125(1)(0.5)(0.5) = 0.03125$  \n",
    "$\\delta^2 = \\delta^3W^2(sigm(z^2)(1-sigm(z^2)) = 0.03125(1)(0.5)(0.5) = 0.0078125$\n",
    "Now we can plug this in: \n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial W^3} &= \\delta^{4} a^3 = 0.125(0) = 0 \\\\\n",
    "\\frac{\\partial C}{\\partial b^3} &= \\delta^{4} = 0.125 \\\\\n",
    "\\frac{\\partial C}{\\partial W^2} &= \\delta^{3} a^2 = 0.03125(0) = 0 \\\\\n",
    "\\frac{\\partial C}{\\partial b^2} &= \\delta^{3} = 0.03125 \\\\\n",
    "\\frac{\\partial C}{\\partial W^1} &= \\delta^{2} x = 0.0078125(0.5) = 0.00390625 \\\\\n",
    "\\frac{\\partial C}{\\partial b^1} &= \\delta^{2} = 0.0078125 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Comment on your observations in **Part C**.  In particular, compare the rate at which weights will be learned in the earlier layers to the later layers.  What would happen if we had an even deeper network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "The rate of change in earlier layers is much lower than that of later layers since the derivative of a sigmoid will repeatedly lower the deltas as we go back in the layers. In a deeper network the effect would be even more pronounced making it so that the earlier layers don't change almost at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [40 points] Problem 3: Building and Training a Feed-Forward Neural Network \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement `forward propagation`, `prediction`, `back propagation`, and a general `train` routine to learn the weights in your network via Stochastic Gradient Descent.  \n",
    "\n",
    "The skeleton for the `Network` class is below. Note that this class is almost identical to the one you worked with in the **Hands-On Neural Network** in-class notebook, so you should look there to remind yourself of the details.   Scroll down to find more information about your tasks as well as unit tests. \n",
    "\n",
    "**Important Note**: In **Problem 4** we'll be using the `Network` class to train a network to do handwritten digit recognition.  Please make sure to utilize vectorized Numpy routines as much as possible, as writing inefficient code here will cause very slow training times in **Problem 4**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:15.724906Z",
     "start_time": "2018-03-13T09:10:15.345793Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        self.a[0] = x\n",
    "        self.z[0] = x\n",
    "        for i in range(1,self.L):\n",
    "            self.z[i] = np.matmul(self.a[i-1],self.W[i-1].T)+self.b[i-1]\n",
    "            self.a[i] = self.g(self.z[i])\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        \n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        for i,x in enumerate(X):\n",
    "            self.forward_prop(x)\n",
    "            yhat[i][np.argmax(self.a[self.L-1])] = 1\n",
    "        \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y,lam):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        self.delta[self.L-1] = self.gradC(self.a[self.L-1],y)*self.g_prime(self.z[self.L-1])\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L-2, -1, -1):\n",
    "            self.delta[ll] = np.matmul(self.W[ll].T,self.delta[ll+1])*self.g_prime(self.z[ll])\n",
    "            pass\n",
    "        for i in range(self.L-1):\n",
    "            self.dW[i] = np.matmul(np.mat(self.delta[i+1]).T,np.mat(self.a[i]))+lam*self.W[i]\n",
    "            self.db[i] = self.delta[i+1]\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                self.back_prop(X_train[ind],y_train[ind],lam)\n",
    "                for i,w in enumerate(self.W):\n",
    "                    self.W[i] = w-eta*self.dW[i]\n",
    "                    self.b[i] = self.b[i]-eta*self.db[i]\n",
    "                \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%10)==1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `forward_prop` function in the `Network` class to implement forward propagation.  Your function should take in a single training example `x` and propagate it forward in the network, setting the activations and activities on the hidden and output layers.  When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:17.610306Z",
     "start_time": "2018-03-13T09:10:17.602365Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testForwardProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict` function in the `Network` class to take in a matrix of features and return a matrix of one-hot-encoded label predictions. Your predictions should correspond to the output neuron with the largest activation.   \n",
    "\n",
    "Recall that our convention for encoding, e.g. the label $y=2$ in a classification problem with possible labels $y \\in \\{0,1,2,3\\}$ is \n",
    "\n",
    "$$\n",
    "y=2 \\quad \\Leftrightarrow \\quad y=\\left[0, 0, 1, 0\\right]\n",
    "$$\n",
    "\n",
    "So the equivalent matrix associated with the labels $y_1=3, y_2=2, y_3=0$ is \n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}{3\\\\2\\\\0}\\end{bmatrix} \\quad \\Leftrightarrow \\quad \n",
    "y = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When you think your `predict` function is working well, execute the following unit test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:19.290278Z",
     "start_time": "2018-03-13T09:10:19.282762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPredict (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: OK, now it's time to implement back propagation.  Complete the function ``back_prop`` in the ``Network`` class to use a single training example to compute the derivatives of the loss function with respect to the weights and the biases. As in the **Hands-On** notebook, you may assume that the loss function for a single training example is given by \n",
    "\n",
    "$$\n",
    "C(y, {\\bf a}^L) = \\frac{1}{2}\\|y - {\\bf a}^L\\|^2  \n",
    "$$\n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:21.407947Z",
     "start_time": "2018-03-13T09:10:21.399745Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testBackProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: OK, now let's actually train a neural net!  Complete the missing code in ``train`` to loop over the training data in random order, call `back_prop` to get the derivatives, and then update the weights and the biases via SGD.  When you think you're done, execute the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:23.048222Z",
     "start_time": "2018-03-13T09:10:23.039936Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testSGD (__main__.TestNN) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: testSGD (__main__.TestNN)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\llair\\Documents\\ML\\CSCI-4622-Machine-Learning\\hmwk\\hmwk04\\tests\\tests.py\", line 67, in testSGD\n",
      "    self.nn.train(self.X_train,self.y_train, eta=0.25, lam=0.0, num_epochs=2, isPrint=False)\n",
      "  File \"<ipython-input-55-bc5d3b47e06c>\", line 149, in train\n",
      "    self.back_prop(X_train[ind],y_train[ind])\n",
      "  File \"<ipython-input-55-bc5d3b47e06c>\", line 112, in back_prop\n",
      "    self.delta[self.L-1] = self.gradC(self.a[self.L-1],y)*self.g_prime(self.z[self.L-1])\n",
      "  File \"<ipython-input-55-bc5d3b47e06c>\", line 50, in g_prime\n",
      "    return self.g(z) * (1.0 - self.g(z))\n",
      "  File \"C:\\Users\\llair\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\", line 309, in __mul__\n",
      "    return N.dot(self, asmatrix(other))\n",
      "ValueError: shapes (1,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Last but not least, we should implement $\\ell$-$2$ regularization.  Modify your `train` function to incorporate regularization of the weights (but **not** the biases) in your SGD update.  As in the Lecture 18 slides, you should assume that the cost function with regularization takes the form \n",
    "\n",
    "$$\n",
    "C_\\lambda = C + \\frac{\\lambda}{2} \\displaystyle\\sum_{w} w^2\n",
    "$$\n",
    "\n",
    "where $\\sum_{w}$ sums over each weight in all layers of the network. Think carefully before you go making large changes to your code.  This modification is much simpler than you think. When you think you're done, execute the following unit test.  (Then go back and execute the test in **Part C** to make sure you didn't break anything.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:25.040053Z",
     "start_time": "2018-03-13T09:10:25.031009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testRegularizedSGD (__main__.TestNN) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: testRegularizedSGD (__main__.TestNN)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\llair\\Documents\\ML\\CSCI-4622-Machine-Learning\\hmwk\\hmwk04\\tests\\tests.py\", line 94, in testRegularizedSGD\n",
      "    self.nn.train(self.X_train,self.y_train, eta=0.25, lam=1.0, num_epochs=2, isPrint=False)\n",
      "  File \"<ipython-input-55-bc5d3b47e06c>\", line 149, in train\n",
      "    self.back_prop(X_train[ind],y_train[ind])\n",
      "  File \"<ipython-input-55-bc5d3b47e06c>\", line 112, in back_prop\n",
      "    self.delta[self.L-1] = self.gradC(self.a[self.L-1],y)*self.g_prime(self.z[self.L-1])\n",
      "  File \"<ipython-input-55-bc5d3b47e06c>\", line 50, in g_prime\n",
      "    return self.g(z) * (1.0 - self.g(z))\n",
      "  File \"C:\\Users\\llair\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\", line 309, in __mul__\n",
      "    return N.dot(self, asmatrix(other))\n",
      "ValueError: shapes (1,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3E\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 4: A Neural Network Classifier for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "In this problem you'll use the Feed-Forward Neural Network framework you wrote in **Problem 3** to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "![Samples of Handwritten Digits](figs/mnist.png \"MNIST Digits\")\n",
    "\n",
    "To keep run times down we'll again only consider the subset of the MNIST data set consisting of the digits $3, 7, 8$ and $9$. \n",
    "\n",
    "**Part A**: Executing the following cells will load training and validation data and plot an example handwritten digit.  Explore the training and validation sets and answer the following questions: \n",
    "\n",
    "- How many pixels are in each image in the data set?  \n",
    "- How do the true labels correspond to the associated one-hot-encoded label vectors? \n",
    "- Give an example of a network architecture with a single hidden layer that is compatible with this data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:27.264422Z",
     "start_time": "2018-03-13T09:10:27.153070Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = pickle.load(gzip.open(\"data/mnist21x21_3789_one_hot.pklz\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:28.454747Z",
     "start_time": "2018-03-13T09:10:28.350656Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAqO0Cg4QqPgCI2CIzQKjtD+DyT5yECHP81jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22303720048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(21,21), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)\n",
    "        \n",
    "training_index = 2\n",
    "label_dict = dict({0:3, 1:7, 2:8, 3:9})\n",
    "view_digit(X_train[training_index], label_dict[np.argmax(y_train[training_index])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Train a network with a single hidden layer containing $30$ neurons on the first $500$ training examples in the training set using a learning rate of $\\eta = 0.01$ for at least $50$ epochs.  What accuracy does your network achieve on the validation set?  Do you see any clear signs of overfitting?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Modify the `Network` class so that it stores the accuracies on the training and validation data every $5$ epochs during the training process. Now increase the number of neurons in the hidden layer to $100$.  On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the learning rates $\\eta = 0.01$, $\\eta = 0.25$ and $\\eta = 1.5$.  Which learning rate seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**:  Now let's see if we can get better results with regularization. Using the best learning rate you found in **Part C**, on a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the regularization strengths $\\lambda = 10^{-6}$, $\\lambda = 10^{-4}$ and $\\lambda = 10^{-2}$.  Which regularization strength seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**:  Now let's see if we can get better results with different network architectures. On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the architecture from **Part D** as well as two other architectures.  Which architecture seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [max 10 points] Extra Credit: Improving Network Performance \n",
    "***\n",
    "\n",
    "See if you can get better performance by exploring advanced techniques.  Things you might try are: \n",
    "\n",
    "- Implementing **Mini-Batch** Stochastic Gradient Descent \n",
    "- Experimenting with different activation functions (like tanh and ReLU)\n",
    "- Experimenting with different loss functions (like cross-entropy or softmax) \n",
    "\n",
    "For more detailed discussion of these techniques it'll be helpful to look at Chapter 3 of [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html). \n",
    "\n",
    "To receive the extra credit you should try at least a couple of the above and clearly describe what worked and what did not.  \n",
    "\n",
    "**Important Note**: Don't do any of these things in the original `Network` class, because you'll almost certainly break the unit tests.  Copy the `Network` class from above and rename it `BetterNetwork` (or something) and modify the new class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
